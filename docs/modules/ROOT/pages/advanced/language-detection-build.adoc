//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

= Building the CharSoup Language Detector

This page documents how the `tika-langdetect-charsoup` language detection model
is trained, the decisions made along the way, and benchmark comparisons against
the existing OpenNLP-based detector. For architecture and API details, see
xref:advanced/language-detection.adoc[Language Detection].

== Training Corpus

Training data is downloaded from
https://huggingface.co/datasets/allenai/MADLAD-400[MADLAD-400]
(Magnusson et al., 2023), a massively multilingual web corpus covering 419
languages derived from Common Crawl. MADLAD is preferred over the Leipzig
Corpora Collection because it carries a permissive Creative Commons license
suitable for redistribution with Tika.

The download script `download_madlad.py` (checked into the repository at
`~/datasets/madlad/`) streams the `_clean_` gzipped JSONL shards directly from
Hugging Face, splits each web document on newlines into sentence-sized units,
applies a minimum-length filter (50 characters), and takes the first 500,000
sentences per language (stopping immediately — no reservoir sampling needed as
MADLAD shards represent diverse web-crawl data).

[source,bash]
----
python download_madlad.py ~/datasets/madlad/data
----

The script writes one file per language:

----
~/datasets/madlad/data/
    eng/
        sentences_madlad.txt   (lineNum<TAB>sentence)
    deu/
        sentences_madlad.txt
    ...
----

MADLAD uses ISO 639-1 two-letter codes for major languages; the script maps
all of these to ISO 639-3 three-letter codes and renames directories
accordingly. The 500k-sentence cap provides ample data for both the language
model (100k used for training) and the common token lists used by `tika-eval`.

=== Deduplication

Deduplication is performed at the Java training-pipeline stage using FNV-1a
64-bit hashing. Web-crawl data has lower duplication rates than Leipzig news
corpora, so a single deduplication pass is sufficient.

[[_language_code_merging]]
=== Language Code Merging

Several languages have multiple ISO 639-3 codes that refer to the same language
or are indistinguishable by character features. These are merged during both
download and training:

[cols="1,1,1"]
|===
| Merged From | Merged To | Note

| `azj` (North Azerbaijani) | `aze` (Azerbaijani) | Code variant
| `cmn` (Mandarin Chinese) | `zho` (Chinese) | Code variant
| `cnr` (Montenegrin) | `srp` (Serbian) | Nearly identical writing system
| `ekk` (Standard Estonian) | `est` (Estonian) | Code variant
| `gug` (Paraguayan Guaraní) | `grn` (Guaraní) | Code variant
| `hbs` (Serbo-Croatian) | `srp` (Serbian) | Umbrella code → canonical
| `khk` (Halh Mongolian) | `mon` (Mongolian) | Code variant
| `lvs` (Standard Latvian) | `lav` (Latvian) | Code variant
| `nor` (Norwegian) | `nob` (Norwegian Bokmål) | Code variant
| `pes` (Iranian Persian) | `fas` (Persian) | Code variant
| `plt` (Plateau Malagasy) | `mlg` (Malagasy) | Code variant
| `prs` (Dari) | `fas` (Persian) | Eastern variety, same written form
| `quz` (Cusco Quechua) | `que` (Quechua) | Code variant
| `swa` (Swahili macrolanguage) | `swh` (Coastal Swahili) | Code variant
| `uzn` (Northern Uzbek) | `uzb` (Uzbek) | Code variant
| `wuu` (Wu Chinese) | `zho` (Chinese) | Code variant
| `yid` (Yiddish macrolanguage) | `ydd` (Eastern Yiddish) | Code variant
| `zsm` (Standard Malay) | `msa` (Malay) | Code variant
|===

The same merge map is maintained in three places and must be kept in sync:

* `download_madlad.py` — `LANG_MERGE_MAP`
* `TrainLanguageModel.java` — `LANG_MERGE_MAP`
* `CommonTokenGenerator.java` — `LANG_MERGE_MAP`

=== Corpus Cleaning

Two data-quality issues were identified during development and addressed in
`CorpusReader.java`:

**Breton (`bre`) noise** — approximately 5% of MADLAD Breton sentences were
French blog posts, identifiable by lines containing three or more consecutive
tildes (`~~~`), a Common Crawl redaction marker. A filter discards any sentence
containing this pattern.

**Dhivehi (`div`) mixed-script headlines** — MADLAD Dhivehi documents
consistently begin with a Latin-script headline followed by Thaana-script body
text, separated by a literal `\n` escape sequence. The pipeline splits on this
separator and treats each segment as a distinct sentence, preventing the Latin
headline from polluting the Thaana training signal. This raised 20-character
accuracy from 32.9% to 95.5%.

=== Filtering Low-Resource Languages

Languages with fewer than **10,000 sentences** after deduplication are excluded.
This threshold ensures enough data for the model to learn useful distributions
even after the mislabel-filtering step.

=== Explicit Language Exclusions

Some languages meet the 10,000-sentence minimum but are explicitly excluded
because they cause one of two problems:

1. **Collateral damage** — their presence causes a closely related majority
   language to drop significantly in accuracy, because character n-grams
   cannot reliably distinguish between them.
2. **Unacceptable own accuracy** — their own detection accuracy is too low to
   be useful, typically because their written form is nearly identical to a
   larger language's.

Exclusion decisions are made by evaluating per-language accuracy on the held-out
test set after a full training run, then examining both own accuracy and any drop
in accuracy for related languages. The threshold for "collateral damage" is
informed by the gap between strict accuracy and group accuracy for the affected
majority language.

[cols="1,1,1,3"]
|===
| Language | Code | Own Accuracy | Reason for Exclusion

| Venetian
| `vec`
| 72.0%
| Italian (`ita`) dropped from its expected range to **83.6%** — a 14.5pp gap
  between strict and group accuracy. Venetian and Italian are indistinguishable
  at the character n-gram level; the collateral damage to a major language is
  unacceptable.

| Tosk Albanian
| `als`
| 69.7%
| Standard Albanian (`sqi`) collapsed to **51.6%** strict accuracy. Adding Tosk
  Albanian effectively broke Albanian detection.

| Madurese
| `mad`
| 9.1%
| Near-random accuracy on 1,003 test sentences. Written in Latin script,
  Madurese is indistinguishable from Javanese and Indonesian at the character
  level.

| Anaang
| `anw`
| 32.5%
| 32.5% accuracy on only 3,036 test sentences — below any useful quality bar,
  with no clear confusable partner to explain the failure.

| Konkani
| `knn`
| 46.2%
| Uses Devanagari script (same as Marathi). The character n-gram profile is
  too similar to Marathi to distinguish reliably; less than half of Konkani
  text is correctly identified.

| Gilaki
| `glk`
| 88.6%
| Northwestern Iranian language whose script profile overlaps with Persian and
  Mazanderani. 88.6% is below the quality bar for included languages.

| Kituba (Monokutuba)
| `mkw`
| 80.1%
| Bantu contact language whose character profile overlaps with Kongo and
  Lingala. 80.1% accuracy is not high enough to justify inclusion.

|===

These exclusions are enforced in `TrainLanguageModel.EXCLUDED_LANGS` and applied
during data preparation, before the training pool is written. If future corpus
growth or model improvements make these languages reliably distinguishable, they
can be removed from the exclusion set.

== Data Splitting Strategy

The data split is designed to maximize use of large corpora while maintaining
stable evaluation sets:

[cols="1,1,1"]
|===
| Split | Size | Preprocessing

| **Test**
| 10% per language (max 20,000)
| Raw (no preprocessing)

| **Dev**
| 10% per language (max 20,000)
| Preprocessed (NFC, lowercase, URL/email stripped)

| **Training pool**
| Remainder
| Preprocessed, stored as per-language files
|===

*Why raw test?* The test set evaluates the full end-to-end pipeline including
preprocessing, feature extraction, and classification. This catches regressions
in any stage.

*Why preprocessed dev?* Dev evaluation happens at every checkpoint and at the end
of every epoch. Preprocessing is deterministic, so pre-applying it once avoids
redundant work and gives the same result as running the full pipeline.

*Why per-language pool files?* Epoch-level resampling (see below) needs to sample
a fixed number of sentences per language. Storing each language in its own file
makes this trivial.

== Training Configuration

* **Hash buckets:** 16,384
* **Feature extractor:** `ScriptAwareFeatureExtractor` with character bigrams,
  skip-bigrams (pairs `(c[i], c[i+2])`), word unigrams, CJK character unigrams,
  and script-aware boundary detection. Skip-bigrams are omitted for CJK scripts.
* **Training sentences per language:** up to 100,000 (sampled from the 500k
  stored per language)
* **Target epoch total:** 5,000,000 sentences per epoch. A flat cap C is
  found by binary search such that Σ min(n_i, C) ≈ 5M. Languages below
  C contribute all their data; languages above C are uniformly capped,
  giving near-uniform class representation without oversampling.
* **Optimizer:** AdamW (lr=0.001, mini-batch 64) × 2 epochs, then
  Hogwild! SGD (lr=0.01→0.001) × 6 max epochs
* **Within-epoch early stopping:** checkpoint every 200K sentences; stop if
  F1 range over last 5 checkpoints < 0.005
* **Across-epoch patience:** stop if no improvement for 2 consecutive epochs
* **JVM:** `-Xmx8g`

=== Epoch-Level Resampling

Rather than training on a single shuffled flat file, each epoch draws a fresh
sample from the training pool:

1. Binary-search for a flat cap C such that `Σ min(n_i, C) ≈ 5,000,000`
2. For each language, randomly sample up to C sentences from its pool file
3. Concatenate all sampled sentences into one epoch file
4. Globally shuffle
5. Train one epoch on the result

This balances data exposure across languages:

* **High-resource languages** (e.g., English with millions of sentences) are
  capped at C per epoch, preventing them from dominating gradient updates
* **Different samples each epoch** ensure the model eventually sees the full
  corpus for high-resource languages across multiple epochs
* **Low-resource languages** (fewer than C sentences) contribute all their
  data every epoch, ensuring they are not underrepresented

== Training Pipeline

=== Pass 1: Initial Training

AdamW for 2 epochs followed by Hogwild! SGD for up to 3 more epochs, each
with epoch-level resampling from the full training pool.

=== Mislabeled Sentence Filtering

The Pass 1 model predicts each sentence in the entire training pool. Sentences
where the prediction does not match the label are removed — unless the
prediction falls within the same confusable language group (e.g., a sentence
labeled `nob` predicted as `nno` is kept).

This filtering is applied once to the full pool, producing a `pool_filtered/`
directory that is used for Pass 2.

=== Pass 2: Retraining on Filtered Data

Same optimizer schedule and resampling strategy, but drawing from the filtered
pool. This typically improves accuracy by 0.3–0.5 percentage points.

=== Final Steps

1. **INT8 quantization** — convert float32 weights to int8 with per-class scales
2. **Evaluate** — test the quantized model on the raw test set (full pipeline)
3. **Export** — write the LDM1 binary model file

== Confusable Language Groups

Certain languages are so similar at the character level that distinguishing them
is inherently difficult. The model defines these groups:

.Peer groups (politically/culturally distinct, linguistically near-identical)
* `nob` / `nno` / `nor` / `dan` — Norwegian Bokmål, Nynorsk, Norwegian (generic), Danish
* `hrv` / `srp` / `bos` / `hbs` — Croatian, Serbian (Latin), Bosnian, Serbo-Croatian
* `msa` / `zlm` / `zsm` / `ind` — Malay / Indonesian variants
* `pes` / `prs` / `fas` — Persian, Dari, Farsi
* `zho` / `cmn` / `wuu` / `yue` — Chinese macro/regional variants
* `tat` / `bak` — Tatar, Bashkir (Cyrillic Turkic)

.Code variant groups (same language, different ISO 639-3 codes)
* `aze` / `azj` — Azerbaijani
* `ekk` / `est` — Estonian
* `lvs` / `lav` — Latvian
* `plt` / `mlg` — Malagasy (Plateau / generic)
* `khk` / `mon` — Mongolian (Khalkha / generic)
* `ydd` / `yid` — Yiddish (Eastern / generic)
* `sme` / `smi` — Sami (Northern / generic)

These groups are used in:

1. **Training** — group-aware mislabel filtering (prevents removing valid
   Bokmål labeled as Nynorsk)
2. **Evaluation** — "group accuracy" metric alongside strict accuracy

== Common Token Lists (tika-eval)

The same MADLAD corpus is used to generate common token frequency lists for
`tika-eval`. The `CommonTokenGenerator` in `tika-eval-core` reads
`sentences_madlad.txt` files and applies:

* `TikaEvalTokenizer` in `COMMON_TOKENS` mode (NFKD normalization, minimum
  length 3, no numbers, no HTML terms)
* The same language merge map and FNV-1a deduplication as the training pipeline

The 500k sentences stored per language provide stable frequency estimates for
the top-30,000 tokens with a minimum document frequency of 10.

[source,bash]
----
java -cp tika-eval/tika-eval-core/target/test-classes:\
tika-eval/tika-eval-core/target/classes:\
tika-langdetect/tika-langdetect-charsoup/target/classes \
    org.apache.tika.eval.core.tokens.tools.CommonTokenGenerator \
    ~/datasets/madlad/data \
    /tmp/common_tokens_new \
    30000 10
----

Arguments: `<corpusDir> <outputDir> [topN] [minDocFreq]`

NOTE: `CommonTokenGenerator` looks for `sentences_madlad.txt` files inside
each language subdirectory.

== Current Build: v3 (MADLAD, in progress)

v3 is the first build trained entirely on MADLAD-400, replacing the
Leipzig Corpora Collection used in v1 and v2. MADLAD's permissive license
makes the training pipeline fully reproducible from publicly available data.

=== v3 Training Configuration

* **Corpus:** MADLAD-400 (500k sentences per language, first-N sampling)
* **Languages:** ~200 (exact count depends on MADLAD coverage after filtering)
* **Hash buckets:** 16,384
* **Feature extractor:** `ScriptAwareFeatureExtractor` — character bigrams,
  skip-bigrams, word unigrams, CJK character unigrams, script-aware boundaries
* **Training sentences per language:** up to 100,000
* **Optimizer:** AdamW (lr=0.001, mini-batch=64) × 2 epochs, then Hogwild!
  SGD (lr=0.01→0.001) × 6 max epochs
* **JVM:** `-Xmx8g`

== Historical Build: v2 (Leipzig)

=== v2 Training Configuration

* **Languages:** 200 (207 corpus languages minus 7 explicit exclusions)
* **Hash buckets:** 8,192
* **Target epoch total:** 5,000,000 sentences. A flat cap is found by binary
  search such that `Σ min(n_i, cap) ≈ 5M`. High-resource languages are
  uniformly downsampled; low-resource languages contribute all their data.
* **Feature extractor:** `ScriptAwareFeatureExtractor` — character bigrams,
  word unigrams, CJK character unigrams, with script-aware boundary detection
* **Optimizer:** AdamW (lr=0.001, mini-batch=64) × 2 epochs, then Hogwild!
  SGD (lr=0.01→0.001) × 6 max epochs
* **Chunk size:** 100,000 lines (50 chunks per 5M epoch for smooth SGD)
* **JVM:** `-Xmx4g`

=== v2 Test Set

[cols="1,1"]
|===
| Split | Sentences

| All | 2,558,571
| Short (≤ 50 chars) | 434,302
| Full (> 50 chars) | 2,124,269
|===

200 languages included. OpenNLP supports 123 of these languages
(2,150,828 shared-language test sentences).

=== v2 Comparison: CharSoup vs OpenNLP (all 200 languages)

OpenNLP returns no prediction for the 77 languages it does not support,
counting as incorrect in the all-languages figures below.

[cols="1,1,1,1"]
|===
| Metric | CharSoup | OpenNLP | Delta

| Accuracy (all)
| **95.24%**
| 76.71%
| +18.5pp

| Accuracy (short ≤ 50 chars)
| **93.51%**
| 69.89%
| +23.6pp

| Accuracy (full sentences)
| **95.59%**
| 78.10%
| +17.5pp

| Group accuracy (all)
| **97.40%**
| 80.74%
| +16.7pp

| Group accuracy (short)
| **95.77%**
| 74.50%
| +21.3pp

| Group accuracy (full)
| **97.73%**
| 82.02%
| +15.7pp
|===

=== v2 Comparison: CharSoup vs OpenNLP (123 shared languages)

Apples-to-apples on the 123 languages both detectors support
(2,150,828 test sentences):

[cols="1,1,1,1"]
|===
| Metric | CharSoup | OpenNLP | Delta

| Accuracy (all)
| **96.42%**
| 91.25%
| +5.2pp

| Accuracy (short ≤ 50 chars)
| **94.44%**
| 87.41%
| +7.0pp

| Accuracy (full sentences)
| **96.80%**
| 91.99%
| +4.8pp
|===

=== v2 Resource Usage

[cols="1,1,1"]
|===
| Metric | CharSoup | OpenNLP

| Throughput
| **~487K sent/sec**
| ~136K sent/sec

| Model heap
| **~2.0 MB**
| ~79.1 MB

| Model file (on disk)
| **1.6 MB**
| 22 MB

| Runtime dependencies
| **None** (tika-core only)
| OpenNLP + model
|===

== Historical Build: v1 (Initial Release)

The numbers below are from the initial model build using 100K sentences per
language. They are preserved here for reference.

=== v1 Corpus

[cols="1,1"]
|===
| Split | Sentences

| Train | 11,570,595
| Dev | 1,446,250
| Test | 1,446,486
|===

199 languages, 49 low-resource languages dropped, 1,928,709 duplicate sentences
removed.

=== v1 Comparison: CharSoup vs OpenNLP

Evaluated on the 123 languages both models support (1,146,734 test sentences):

[cols="1,1,1,1"]
|===
| Metric | CharSoup | OpenNLP | Delta

| Accuracy (all)
| **96.00%**
| 89.68%
| +6.3pp

| Accuracy (short ≤ 50 chars)
| **94.20%**
| 86.23%
| +8.0pp

| Accuracy (full sentences)
| **96.38%**
| 90.39%
| +6.0pp
|===

=== v1 Resource Usage

[cols="1,1,1"]
|===
| Metric | CharSoup | OpenNLP

| Throughput
| **~510K sent/sec**
| ~135K sent/sec

| Model heap
| **~2 MB**
| ~79 MB

| Model file (on disk)
| **1.6 MB**
| 22 MB

| Runtime dependencies
| **None** (tika-core only)
| OpenNLP + model
|===

== References

For the academic references behind the techniques used in training and inference,
see the xref:advanced/language-detection.adoc#_references[References section] of
the main language detection page.
