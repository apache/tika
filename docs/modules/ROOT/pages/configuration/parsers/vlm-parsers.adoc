//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

= VLM (Vision-Language Model) Parsers

Tika includes a family of parsers that delegate OCR and document understanding
to remote Vision-Language Model (VLM) endpoints. These parsers send images
(or PDFs) to an external API and convert the model's markdown response into
structured XHTML.

Three implementations are provided out of the box:

[cols="1,2,1,1"]
|===
|Parser |Endpoint |Config key |SPI auto-loaded?

|`OpenAIVLMParser`
|Any OpenAI-compatible chat completions endpoint (vLLM, Ollama, local FastAPI, OpenAI)
|`openai-vlm-parser`
|Yes

|`ClaudeVLMParser`
|Anthropic Messages API
|`claude-vlm-parser`
|No

|`GeminiVLMParser`
|Google Gemini `generateContent` API
|`gemini-vlm-parser`
|No
|===

`OpenAIVLMParser` is the only parser loaded by the default parser via SPI.
`ClaudeVLMParser` and `GeminiVLMParser` must be explicitly added to your
configuration.

== Supported input types

All three parsers handle standard OCR image types (`image/ocr-png`,
`image/ocr-jpeg`, etc.).  `ClaudeVLMParser` and `GeminiVLMParser`
additionally declare `application/pdf` support, meaning they can process
PDFs natively using the model's vision capabilities.

== Module dependency

The VLM parsers live in the `tika-parser-vlm-ocr-module` artifact. Add it
to your project:

[source,xml]
----
<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-parser-vlm-ocr-module</artifactId>
  <version>${tika.version}</version>
</dependency>
----

TIP: To run a local open-source VLM without cloud API keys, see
xref:advanced/local-vlm-server.adoc[Running a Local VLM Server].

== OpenAI-compatible (vLLM, Ollama, etc.)

=== Basic Configuration

[source,json]
----
include::example$openai-vlm-basic.json[]
----

=== Full Configuration

[source,json]
----
include::example$openai-vlm-full.json[]
----

The `OpenAIVLMParser` works with any server that exposes an
`/v1/chat/completions` endpoint in the OpenAI format. This includes:

* https://github.com/vllm-project/vllm[vLLM]
* https://ollama.com[Ollama]
* A local FastAPI / Flask wrapper around a Hugging Face model
* OpenAI itself

Authentication uses a standard `Authorization: Bearer <apiKey>` header.
Leave `apiKey` empty to skip authentication (typical for local servers).

== Anthropic Claude

=== Basic Configuration

[source,json]
----
include::example$claude-vlm-basic.json[]
----

=== Full Configuration

[source,json]
----
include::example$claude-vlm-full.json[]
----

The `ClaudeVLMParser` uses the Anthropic
https://docs.anthropic.com/en/api/messages[Messages API].
Authentication uses the `x-api-key` header (not Bearer). The required
`anthropic-version` header is sent automatically.

Claude handles images and PDFs natively. For images, the content block
type is `image`; for PDFs it is `document`. The parser detects the
correct type from the input MIME type.

== Google Gemini

=== Basic Configuration

[source,json]
----
include::example$gemini-vlm-basic.json[]
----

=== Full Configuration

[source,json]
----
include::example$gemini-vlm-full.json[]
----

The `GeminiVLMParser` targets the Google
https://ai.google.dev/api/generate-content[Gemini `generateContent`]
endpoint. The API key is passed as a `key` query parameter.

Change `baseUrl` if you are using Vertex AI or a proxy.

== Using a VLM parser for PDF parsing

Claude and Gemini can process entire PDFs with their vision capabilities.
To route PDFs to a VLM parser instead of the default `PDFParser`, exclude
the default and add the VLM parser:

[source,json]
----
include::example$vlm-pdf-parsing.json[]
----

You can substitute `gemini-vlm-parser` for `claude-vlm-parser` above.

== Configuration options reference

All three parsers share the same configuration POJO (`VLMOCRConfig`):

[cols="2,1,3"]
|===
|Property |Default |Description

|`baseUrl`
|varies by parser
|Base URL of the API endpoint (no trailing slash).

|`model`
|varies by parser
|Model identifier sent in the API request.

|`prompt`
|_(markdown extraction prompt)_
|The text prompt sent alongside the image or document.

|`maxTokens`
|`4096`
|Maximum tokens the model may generate.

|`timeoutSeconds`
|`300`
|HTTP read timeout in seconds.

|`apiKey`
|`""` (empty)
|API key. Format depends on the parser (Bearer header, x-api-key header,
or query parameter).

|`inlineContent`
|`true`
|When parsing inline images (embedded resource type `INLINE`), write OCR
text into the parent document's content stream. Mirrors
`TesseractOCRParser` inline behaviour.

|`skipOcr`
|`false`
|Runtime kill-switch to disable the parser entirely.

|`minFileSizeToOcr`
|`0`
|Minimum input file size in bytes.

|`maxFileSizeToOcr`
|`52428800` (50 MB)
|Maximum input file size in bytes.
|===

== Markdown-to-XHTML conversion

The VLM's text response is expected to be markdown. Tika parses it using
https://github.com/commonmark/commonmark-java[commonmark-java] and emits
proper XHTML elements (`<h1>`, `<p>`, `<table>`, `<b>`, `<i>`, etc.)
instead of dumping raw text. GFM tables and strikethrough are supported.

== Per-request configuration

You can override configuration per-request by setting a `VLMOCRConfig`
instance on the `ParseContext`:

[source,java]
----
VLMOCRConfig override = new VLMOCRConfig();
override.setModel("claude-opus-4-20250514");
override.setMaxTokens(8192);

ParseContext context = new ParseContext();
context.set(VLMOCRConfig.class, override);
----

@since Apache Tika 4.0
